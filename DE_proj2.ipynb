{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5993ba",
   "metadata": {},
   "source": [
    "#### DE_A Comprehensive ETL Workflow with Python for Data Engineers\n",
    "\n",
    "#### Author:  Aditya Balasubramaniam  \n",
    "#### Project Name:  ETL Data Pipeline â€“ Multi-Format Extraction & Transformation  \n",
    "#### Last Updated:  August 2025\n",
    "\n",
    "####  Project Overview : This project demonstrates a complete ETL (Extract, Transform, Load) process using Python to handle data from multiple file formats (CSV, JSON, XML).  It performs extraction, transformation (unit conversions), deduplication, and final loading to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5d2a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "beb758f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging setup\n",
    "log_file_path = 'log_file.txt'\n",
    "def log_progress(message):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        f.write(f\"{timestamp} - {message}\\n\")\n",
    "    print(f\"{timestamp} - {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26caa95c",
   "metadata": {},
   "source": [
    "# Step 1: Download and unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a79c98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:13:57 - Download complete.\n",
      "2025-08-05 09:13:57 - Extraction complete and zip file removed.\n"
     ]
    }
   ],
   "source": [
    "#extracting the dataset from the url link\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0221EN-SkillsNetwork/labs/module%206/Lab%20-%20Extract%20Transform%20Load/data/source.zip'\n",
    "zip_file = 'source.zip'\n",
    "extract_to = './unzipped_folder'\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(zip_file, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "log_progress(\"Download complete.\")\n",
    "\n",
    "if not os.path.exists(extract_to):\n",
    "    os.makedirs(extract_to)\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "os.remove(zip_file)\n",
    "\n",
    "log_progress(\"Extraction complete and zip file removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25d3de",
   "metadata": {},
   "source": [
    "# Step 2: Extract data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1eab974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:13:57 - CSV data extracted.\n"
     ]
    }
   ],
   "source": [
    "csv_files = glob.glob(os.path.join(extract_to, '*.csv'))\n",
    "df_csv = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "log_progress(\"CSV data extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57cc8de",
   "metadata": {},
   "source": [
    "# Step 3: Extract data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb137f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:13:57 - JSON data extracted.\n"
     ]
    }
   ],
   "source": [
    "json_files = glob.glob(os.path.join(extract_to, '*.json'))\n",
    "df_json = pd.concat([pd.read_json(file, lines=True) for file in json_files], ignore_index=True)\n",
    "log_progress(\"JSON data extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d4ab0",
   "metadata": {},
   "source": [
    "# Step 4: Extract data from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "854336de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:13:57 - XML data extracted.\n"
     ]
    }
   ],
   "source": [
    "xml_files = glob.glob(os.path.join(extract_to, '*.xml'))\n",
    "xml_data = []\n",
    "for file in xml_files:\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        row = {elem.tag: elem.text for elem in child}\n",
    "        xml_data.append(row)\n",
    "df_xml = pd.DataFrame(xml_data)\n",
    "log_progress(\"XML data extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872b34c",
   "metadata": {},
   "source": [
    "# Step 5: Combine all extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "68063fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:13:57 - DataFrames combined.\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.concat([df_csv, df_json, df_xml], ignore_index=True)\n",
    "log_progress(\"DataFrames combined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc0c6a",
   "metadata": {},
   "source": [
    "# Step 6: Clean and Transform with Rounded Unit Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9bd001fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:13:57 - Transformation complete (converted, rounded, dropped raw columns, and removed duplicates).\n"
     ]
    }
   ],
   "source": [
    "df_all['height'] = pd.to_numeric(df_all['height'], errors='coerce')\n",
    "df_all['weight'] = pd.to_numeric(df_all['weight'], errors='coerce')\n",
    "\n",
    "df_all['height_meters'] = (df_all['height'] * 0.0254).round(2)\n",
    "df_all['weight_kgs'] = (df_all['weight'] * 0.453592).round(2)\n",
    "\n",
    "df_all.drop(columns=['height', 'weight'], inplace=True)\n",
    "\n",
    "df_all.drop_duplicates(inplace=True)\n",
    "\n",
    "log_progress(\"Transformation complete (converted, rounded, dropped raw columns, and removed duplicates).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b61ba4",
   "metadata": {},
   "source": [
    "## Step 7: Save to CSV file format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cef5172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:13:57 - Final data saved to transformed_data.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ETL process is complete.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.to_csv('transformed_data.csv', index=False)\n",
    "log_progress(\"Final data saved to transformed_data.csv\")\n",
    "\n",
    "'ETL process is complete.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
